{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a31551",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d54d2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the MovieLens dataset\n",
    "ratings_data = pd.read_csv('ratings.csv')\n",
    "movies_data = pd.read_csv('movies.csv')\n",
    "\n",
    "# Merge the two datasets on the 'movieId' column\n",
    "data = pd.merge(ratings_data, movies_data, on='movieId')\n",
    "\n",
    "# Group the data by 'userId' and create a pivot table with 'movieId' as rows and 'userId' as columns\n",
    "ratings_matrix = data.pivot_table(index='movieId', columns='userId', values='rating')\n",
    "\n",
    "# Replace any missing values with zeros\n",
    "ratings_matrix = ratings_matrix.fillna(0)\n",
    "\n",
    "# Define a function to calculate the cross-convolution filter for a given movie\n",
    "def cross_conv_filter(window_size=5):\n",
    "    # Get the movie id from the user via the keyboard\n",
    "    movie_id = int(input(\"Enter a movie id: \"))\n",
    "    \n",
    "    # Get the ratings for the given movie\n",
    "    movie_ratings = ratings_matrix.loc[movie_id].values\n",
    "    \n",
    "    # Pad the ratings array with zeros on both sides\n",
    "    padded_ratings = np.pad(movie_ratings, (window_size//2, window_size//2), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Define the filters\n",
    "    h_filter = np.array([-1, 0, 1])\n",
    "    v_filter = np.array([[-1], [0], [1]])\n",
    "    \n",
    "    # Calculate the horizontal convolution\n",
    "    h_convolved_ratings = np.convolve(padded_ratings, h_filter, mode='valid')\n",
    "    \n",
    "    # Pad the horizontal convolved ratings array with zeros on both sides\n",
    "    padded_h_convolved_ratings = np.pad(h_convolved_ratings, (window_size//2, window_size//2), mode='constant', constant_values=0)\n",
    "    \n",
    "    # Calculate the vertical convolution\n",
    "    v_convolved_ratings = np.convolve(padded_h_convolved_ratings, v_filter.flatten(), mode='valid')\n",
    "    \n",
    "    # Get the top 10 highest values\n",
    "    top_indices = np.argsort(v_convolved_ratings)[-10:]\n",
    "    \n",
    "    # Get the movie titles and scores\n",
    "    movie_indices = ratings_matrix.index[top_indices]\n",
    "    movie_scores = v_convolved_ratings[top_indices]\n",
    "    movie_titles = movies_data[movies_data['movieId'].isin(movie_indices)]['title'].values\n",
    "    \n",
    "    # Print the recommended movies\n",
    "    print('Recommended movies for movieId {}:'.format(movie_id))\n",
    "    for i in range(len(movie_indices)):\n",
    "        print('{}, score: {:.2f}'.format(movie_titles[i], movie_scores[i]))\n",
    "\n",
    "# Example usage\n",
    "cross_conv_filter()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95545eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global_max_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe4831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "# Load data\n",
    "movies = pd.read_csv(\n",
    "    'movies.dat', \n",
    "    sep = '::', \n",
    "    header = None, \n",
    "    engine = 'python', \n",
    "    encoding = 'latin-1',\n",
    "    names=[\"movieId\", \"Title\", \"Genres\"]\n",
    ")\n",
    "ratings = pd.read_csv(\n",
    "    'ratings.dat', \n",
    "    sep = '::', \n",
    "    header = None, \n",
    "    engine = 'python', \n",
    "    encoding = 'latin-1',\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"Timestamp\"]\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "unique_movie_ids = ratings['movieId'].unique()\n",
    "unique_user_ids = ratings['userId'].unique()\n",
    "\n",
    "# Create a mapping from movie and user ids to their respective indices in the input tensors\n",
    "movie_to_idx = {movie_id: i for i, movie_id in enumerate(unique_movie_ids)}\n",
    "user_to_idx = {user_id: i for i, user_id in enumerate(unique_user_ids)}\n",
    "\n",
    "# Add the indices to the ratings dataframe\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df = ratings.sample(frac=0.8, random_state=42)\n",
    "test_df = ratings.drop(train_df.index)\n",
    "\n",
    "# Define model architecture\n",
    "n_users = len(user_to_idx)\n",
    "n_movies = len(movie_to_idx)\n",
    "n_factors = 50\n",
    "\n",
    "# Input layers\n",
    "movie_input = keras.layers.Input(shape=[1], name='movie')\n",
    "user_input = keras.layers.Input(shape=[1], name='user')\n",
    "\n",
    "# Embedding layers\n",
    "movie_embedding = keras.layers.Embedding(n_movies, n_factors, name='movie_embedding')(movie_input)\n",
    "user_embedding = keras.layers.Embedding(n_users, n_factors, name='user_embedding')(user_input)\n",
    "\n",
    "# Reshape embedding layers for compatibility with convolutional layers\n",
    "movie_embedding_reshaped = keras.layers.Reshape((1, n_factors, 1))(movie_embedding)\n",
    "user_embedding_reshaped = keras.layers.Reshape((1, n_factors, 1))(user_embedding)\n",
    "\n",
    "# Convolutional layers\n",
    "conv1 = keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(movie_embedding_reshaped)\n",
    "conv2 = keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(user_embedding_reshaped)\n",
    "merged = keras.layers.concatenate([conv1, conv2])\n",
    "\n",
    "# Max pooling layer\n",
    "pool = keras.layers.MaxPooling2D(pool_size=(1, 2))(merged)\n",
    "\n",
    "# Flatten layer\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "\n",
    "# Output layer\n",
    "output = keras.layers.Dense(1, activation='relu')(flatten)\n",
    "\n",
    "# Define model\n",
    "model = keras.Model(inputs=[movie_input, user_input], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Train the model on the training set\n",
    "history = model.fit(\n",
    "    [train_df['movie_idx'].values, train_df['user_idx'].values], train_df['rating'].values,\n",
    "    epochs=10, batch_size=32, verbose=1,\n",
    "    validation_data=([test_df['movie_idx'].values, test_df['user_idx'].values], test_df['rating'].values),\n",
    "    callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate([test_df['movie_idx'].values, test_df['user_idx'].values], test_df['rating'].values)\n",
    "test_rmse = np.sqrt(test_loss)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test MSE:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a7e100",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'b')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cff7910",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Global_average_pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85851118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, callbacks\n",
    "\n",
    "# Load data\n",
    "movies = pd.read_csv(\n",
    "    'movies.dat', \n",
    "    sep = '::', \n",
    "    header = None, \n",
    "    engine = 'python', \n",
    "    encoding = 'latin-1',\n",
    "    names=[\"movieId\", \"Title\", \"Genres\"]\n",
    ")\n",
    "ratings = pd.read_csv(\n",
    "    'ratings.dat', \n",
    "    sep = '::', \n",
    "    header = None, \n",
    "    engine = 'python', \n",
    "    encoding = 'latin-1',\n",
    "    names=[\"userId\", \"movieId\", \"rating\", \"Timestamp\"]\n",
    ")\n",
    "\n",
    "# Preprocess data\n",
    "unique_movie_ids = ratings['movieId'].unique()\n",
    "unique_user_ids = ratings['userId'].unique()\n",
    "\n",
    "# Create a mapping from movie and user ids to their respective indices in the input tensors\n",
    "movie_to_idx = {movie_id: i for i, movie_id in enumerate(unique_movie_ids)}\n",
    "user_to_idx = {user_id: i for i, user_id in enumerate(unique_user_ids)}\n",
    "\n",
    "# Add the indices to the ratings dataframe\n",
    "ratings['movie_idx'] = ratings['movieId'].map(movie_to_idx)\n",
    "ratings['user_idx'] = ratings['userId'].map(user_to_idx)\n",
    "\n",
    "# Split data into training and test sets\n",
    "train_df = ratings.sample(frac=0.8, random_state=42)\n",
    "test_df = ratings.drop(train_df.index)\n",
    "\n",
    "# Define model architecture\n",
    "n_users = len(user_to_idx)\n",
    "n_movies = len(movie_to_idx)\n",
    "n_factors = 50\n",
    "\n",
    "# Input layers\n",
    "movie_input = keras.layers.Input(shape=[1], name='movie')\n",
    "user_input = keras.layers.Input(shape=[1], name='user')\n",
    "\n",
    "# Embedding layers\n",
    "movie_embedding = keras.layers.Embedding(n_movies, n_factors, name='movie_embedding')(movie_input)\n",
    "user_embedding = keras.layers.Embedding(n_users, n_factors, name='user_embedding')(user_input)\n",
    "\n",
    "# Reshape embedding layers for compatibility with convolutional layers\n",
    "movie_embedding_reshaped = keras.layers.Reshape((1, n_factors, 1))(movie_embedding)\n",
    "user_embedding_reshaped = keras.layers.Reshape((1, n_factors, 1))(user_embedding)\n",
    "\n",
    "# Convolutional layers\n",
    "conv1 = keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding='same')(movie_embedding_reshaped)\n",
    "conv2 = keras.layers.Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same')(user_embedding_reshaped)\n",
    "merged = keras.layers.concatenate([conv1, conv2])\n",
    "\n",
    "# Average pooling layer\n",
    "pool = keras.layers.AveragePooling2D(pool_size=(1, 2))(merged)\n",
    "\n",
    "# Flatten layer\n",
    "flatten = keras.layers.Flatten()(pool)\n",
    "\n",
    "# Output layer\n",
    "output = keras.layers.Dense(1, activation='relu')(flatten)\n",
    "\n",
    "# Define model\n",
    "model = keras.Model(inputs=[movie_input, user_input], outputs=output)\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# Train the model on the training set\n",
    "history = model.fit(\n",
    "    [train_df['movie_idx'].values, train_df['user_idx'].values], train_df['rating'].values,\n",
    "    epochs=10, batch_size=32, verbose=1,\n",
    "    validation_data=([test_df['movie_idx'].values, test_df['user_idx'].values], test_df['rating'].values),\n",
    "    callbacks=[callbacks.EarlyStopping(patience=3, restore_best_weights=True)]\n",
    ")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate([test_df['movie_idx'].values, test_df['user_idx'].values], test_df['rating'].values)\n",
    "test_rmse = np.sqrt(test_loss)\n",
    "print(\"Test RMSE:\", test_rmse)\n",
    "print(\"Test MSE:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419b730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get training and test loss histories\n",
    "training_loss = history.history['loss']\n",
    "\n",
    "# Create count of the number of epochs\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "# Visualize loss history\n",
    "plt.plot(epoch_count, training_loss, 'r')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
